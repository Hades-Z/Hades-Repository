{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Logistic Regression\n",
    "#### 背景：\n",
    "逻辑回归是经典的二分类机器学习模型，凭借可解释性强等优点，现在仍是主流二分类问题解决方案。\n",
    "\n",
    "逻辑回归模型，首先将LinearRegression结果导入Sigmoid函数，实现对样本为positive概率的预测；\n",
    "然后将预测概率与threshold进行比较，大于threshold预测为positive，反之为negative。\n",
    "\n",
    "#### 目的：\n",
    "1. 借助线性回归模型，探究主要数据预处理方法作用，及其对模型结果的影响\n",
    "2. 探究矩阵运算对模型计算效率的提升作用\n",
    "3. 探究模型过拟合的抑制方法和实用策略\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 环境初始化\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"  # 执行全部行输出命令"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 数据准备\n",
    "浏览数据集描述信息后发现：\n",
    "1. 数据集一共有13个特征、506个样本，且数据集中没有缺失数据；\n",
    "2. 13个特征中，数值型特征11个，类别型特征2个（CHAS,RAD）"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "from sklearn.datasets import load_boston\n",
    "data = load_boston()\n",
    "print(data['DESCR'])\n",
    "# data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 数据集特征探索\n",
    "df = pd.DataFrame(data['data'],columns=data['feature_names'])\n",
    "df['target']=data['target']\n",
    "df.info(); df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 数据预处理\n",
    "1. 数值型特征与类别型特征分离\n",
    "2. 缺失值填充\n",
    "3. 重复值处理\n",
    "4. 异常值检测\n",
    "5. 类别型特征编码\n",
    "6. 数值型特征转换\n",
    "7. 特征衍生与降维（可选）\n",
    "8. 特征筛选（慎重）\n",
    "9. 数据集分割"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 数值型特征与类别型特征分离\n",
    "df_cat = df[['CHAS','RAD']].astype('int').astype('category')\n",
    "df_num = df.drop(columns=['CHAS','RAD','target'])\n",
    "# print(df_cat.shape); print(df_num.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 类别型特征编码主要有 ordinal encoding 和 one-hot encoding 两种方法\n",
    "\n",
    "1. ordinal encoding编码\n",
    "适用于处理类别间具有大小关系的顺序型类别特征，它按类别大小关系，给其赋予一个从1到n的正整数数值ID，将类别型特征转化成数值型哑变量。\n",
    "\n",
    "2. one-hot encoding\n",
    "适用于处理类别间不具有大小关系的分类型类别特征，它按特征值类别数量产生一个n维0-1稀疏向量，每种特征值由向量中对应维度为1、其它维度为0表示。\n",
    "**使得任意两不同类别的编码向量之差相等，使得模型学习时，可以对每种类别一直同仁。**\n",
    "\n",
    "3. 网传可以通过二进制编码的方法，转化分类型类别特征，得到与one-hot encoding同样的效果，并且向量维数少于one-hot、节省存储空间。\n",
    "**这种说法是完全不对的，因为二进制编码与ordinal encoding的本质相同：编码向量间存在大小关系，任意不同类别编码向量间差值不等，模型学习时不能对特征的每种类别一视同仁**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 类别型特征编码\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "onehot = OneHotEncoder()\n",
    "nar_cat = onehot.fit_transform(df_cat).toarray()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 数值型特征描述性统计分析\n",
    "df_num.describe()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 数值型特征转换主要有Feature Scaling和Non-linear transformation两大类方法\n",
    "\n",
    "**Feature Scaling**\n",
    "\n",
    "标准化\n",
    "\n",
    "定义：对不同维度数值特征做线性变换，使得不同度量之间的特征具有可比性，它不改变原始数据的分布。\n",
    "\n",
    "常用方法：\n",
    "1. Normalization (min-max normalization)\n",
    "2. Standardization(Z-score normalization)\n",
    "\n",
    "Normalization是一个源于统计学的概念，它包含数据预处理中常说的Standardization和Normalization两个概念，它们都是Feature Scaling的一种方法。\n",
    "\n",
    "*特别注意的是，稀疏数据一般不要执行标准化转换，因为Normalization会将一个多数元素为0的稀疏向量转化成一个多数元素不为0的密集特征向量，这会给模型运算带来巨大的负担。*\n",
    "\n",
    "**Non-linear transformation**\n",
    "\n",
    "归一化\n",
    "\n",
    "定义：对不同维度数值特征做非线性变换，使各个特征维度对目标函数的影响权重一致，即将那些扁平分布的特征数据伸缩变换成类圆形分布，会改变原始数据的分布。\n",
    "\n",
    "常用方法：\n",
    "1. L2范数归一化\n",
    "2. 对数变换\n",
    "3. 指数变换"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 数值型特征缩放\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "standardized = StandardScaler()\n",
    "nar_num = standardized.fit_transform(df_num)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 特征筛选"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 特征线性相关性筛选\n",
    "col_cat = ['CHAS'+str(i) for i in df_cat.CHAS.drop_duplicates()]+['RAD'+str(i) for i in df_cat.RAD.drop_duplicates()]\n",
    "col_num = list(df_num.columns)\n",
    "col = col_num + col_cat + ['target']\n",
    "df_rul = pd.DataFrame(np.concatenate((nar_num, nar_cat, df[['target']].values), axis=1), columns=col)\n",
    "df_rul.corr()['target'].sort_values(ascending=False)\n",
    "\n",
    "# 获取标签数据集\n",
    "y = df_rul[['target']].values.reshape(len(data['target']),1)\n",
    "\n",
    "# 获取特征数据集1：保留全部特征\n",
    "X_all = df_rul.drop(columns='target').values\n",
    "\n",
    "# 获取特征数据集2：去除低相关性特征，保留中、高相关性特征\n",
    "X_lcM = np.concatenate((nar_num, df_rul[['RAD24']].values), axis=1)\n",
    "\n",
    "# 获取特征数据集3：仅保留高相关性特征\n",
    "X_lcH = df_rul[['RM','PTRATIO','LSTAT']].values\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 数据集分割"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 定义分割函数\n",
    "def data_split(data, test_ratio=0.2, val_ratio=0, index=0):\n",
    "    if isinstance(index,int): index = np.random.choice(range(len(data)), size=len(data), replace=False)\n",
    "    train_index = index[:int(len(data)*(1-val_ratio-test_ratio))]\n",
    "    val_index = index[int(len(data)*(1-val_ratio-test_ratio)):int(len(data)*(1-test_ratio))]\n",
    "    test_index = index[int(len(data)*(1-test_ratio)):]\n",
    "    return data[train_index], data[test_index], data[val_index], index\n",
    "#\n",
    "y_train,y_test,_,_ = data_split(y,test_ratio=0.2)\n",
    "X_all_train,X_all_test,_,X_index = data_split(X_all,test_ratio=0.2)\n",
    "X_lcM_train,X_lcM_test,_,_ = data_split(X_lcM,test_ratio=0.2,index=X_index)\n",
    "X_lcH_train,X_lcH_test,_,_ = data_split(X_lcH,test_ratio=0.2,index=X_index)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 模型定义\n",
    "def LinReg_train(X,y,num_epochs,lr,lam,patient):\n",
    "    num_sample,num_feature = X.shape\n",
    "    # 初始化\n",
    "    W_rec = [np.random.normal(0,1,(1,num_feature))]\n",
    "    b_rec = [0]\n",
    "    loss = [0]              # 每迭代损失函数值记录器\n",
    "    loss_diff = [np.inf]    # 每迭代损失函数下降步长记录器\n",
    "    j = 0                   # patient消耗程度记录器\n",
    "    # 训练\n",
    "    for i in range(num_epochs):\n",
    "        W = W_rec[i]; b = b_rec[i]\n",
    "        y_hat = X.dot(W.T)+b\n",
    "        # MSE & L2\n",
    "        # ls = float(np.sum((y_hat-y)**2)/(2*num_sample) + (lam/2)*W.dot(W.T))\n",
    "        ls = np.sum((y_hat-y)**2)/(2*num_sample)\n",
    "        # ls = (y_hat-y).dot((y_hat-y).T)/(2*num_sample)\n",
    "        loss.append(ls)\n",
    "        # 提前终止\n",
    "        loss_diff.append(float(abs(loss[-1]-loss[-2])))\n",
    "        if loss_diff[-1]>=min(loss_diff[:-1]): j+=1\n",
    "        if j==patient:\n",
    "            minLoss_index = loss.index(min(loss[1:]))\n",
    "            return loss[1:minLoss_index+1], W_rec[minLoss_index-1], b_rec[minLoss_index-1]\n",
    "        # 优化（GradientDescent & L2）\n",
    "        W = W-lr*(y_hat-y).T.dot(X)/num_sample - lr*lam*W\n",
    "        # W = W-lr*(y_hat-y).T.dot(X)/num_sample\n",
    "        b = b-lr*np.mean(y_hat-y)\n",
    "        W_rec.append(W); b_rec.append(b)\n",
    "    return loss[1:], W_rec[-1], b_rec[-1]\n",
    "\n",
    "# 预测\n",
    "def LinReg_price(X,y,W,b,lam):\n",
    "    num_sample,_ = X.shape\n",
    "    y_hat = X.dot(W.T)+b\n",
    "    # MSE\n",
    "    ls = np.sum((y_hat-y)**2)/(2*num_sample)\n",
    "    # ls = (y_hat-y).dot((y_hat-y).T)/(2*num_sample)\n",
    "    return y_hat, ls\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 抑制过拟合\n",
    "正则化和特征筛选是抑制过拟合的常用方法；\n",
    "\n",
    "其中，正则化常用实现方法有参数L2/L1范数惩罚和提前终止等，它们可以组合使用也可以单独使用。\n",
    "\n",
    "特征选择方法十分丰富，其目的是选出对预测标签最重要的一些特征，常用方法有线性相关性筛选、决策树/随机森林节点排序筛选和借助参数L1范数惩罚实现\n",
    "\n",
    "基于本项目的试验结果，综合来看过拟合抑制方法的推荐排序是首先使用提前终止和参数L2范数惩罚，如果效果能不满足要求再谨慎使用特征筛选。\n",
    "\n",
    "这是因为，提前终止没有需要校调的超参数，且在抑制过拟合的同时不会带来任何负面影响，但花无两样红，不得不说的是提前终止的过拟合抑制能力也远弱于另两种方法；\n",
    "\n",
    "另一方面，谨慎使用特征筛选的原因是，特征筛选会损失信息，同时本项目的试验结果显示，同时使用提前终止、参数惩罚时，特征筛选会增加一些模型预测结果的不稳定性（重复执行程序，针对不同随机抽样数据，模型评估指标波动程度增加）；\n",
    "\n",
    "通过合理设置参数L2范数惩罚项系数，可以在充分抑制过拟合的同时不损失特征信息、避免结果不稳定，所以结论：优先进行范数惩罚，最后再考虑特征筛选。\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 超参数定义\n",
    "num_epochs =1000000 #训练迭代次数\n",
    "lr = 0.03           #学习率\n",
    "lam = 10            #正则化惩罚系数\n",
    "patient = 10        #提前停止容忍度\n",
    "Num = 100          #重复试验次数\n",
    "\n",
    "#\n",
    "def Fun1(X_train,y_train,X_test,y_test,num_epochs,lr,lam,patient,Num):\n",
    "    # 初始化\n",
    "    ls=0; loss_=0\n",
    "    #\n",
    "    for _ in range(Num):\n",
    "        # 训练\n",
    "        loss_CV,W,b = LinReg_train(X_train,y_train,num_epochs,lr,lam,patient)\n",
    "        # 模型效果评估\n",
    "        ls+=loss_CV[-1]\n",
    "        # 预测\n",
    "        y_hat,loss = LinReg_price(X_test,y_test,W,b,3)\n",
    "        loss_+=loss\n",
    "    print(ls/Num, loss_/Num)\n",
    "\n",
    "# 全特征\n",
    "Fun1(X_all_train,y_train,X_all_test,y_test,num_epochs,lr,lam,patient,Num)\n",
    "# 中、高相关特征\n",
    "Fun1(X_lcM_train,y_train,X_lcM_test,y_test,num_epochs,lr,lam,patient,Num)\n",
    "# 仅含高相关特征\n",
    "Fun1(X_lcH_train,y_train,X_lcH_test,y_test,num_epochs,lr,lam,patient,Num)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-71aba385",
   "language": "python",
   "display_name": "PyCharm (Python Project)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}